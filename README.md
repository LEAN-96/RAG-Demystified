# RAG-Demystified

Are you intrigued by the concept of **Retrieval-Augmented-Generation (RAG)** but find it intimidating due to its association with AI and machine learning? Look no further! RAG-Demystified is your gateway to understanding this groundbreaking technology without the need for prior expertise in AI.

In this repository, our mission is simple: to break down RAG into easily digestible explanations that anyone can understand. Whether you're a student exploring the realms of artificial intelligence, a professional seeking to broaden your knowledge, or simply curious about the latest advancements in technology, RAG Demystified is designed with you in mind.

But we don't stop there. Recognizing that the AI community often assumes familiarity with concepts like **Large Language Models (LLMs)** and the transformer architecture, we go the extra mile to ensure that even the basics are accessible to everyone. No technical jargon or complex terminology here â€“ just straightforward explanations and illustrative examples.

Join us on a journey of discovery as we unravel the mysteries of RAG and provide you with the tools to navigate this exciting field with confidence. By the end of your exploration with RAG-Demystified, you'll have a solid understanding of not only RAG itself but also the foundational concepts that underpin it. Welcome to RAG-Demystified, where complexity meets clarity.


# Fundamentals of Large Language Models (LLMs):


# Limitations of Large Language Models (LLMs):


# Introduction to RAG:
![image](https://github.com/LEAN-96/RAG-Demystified/assets/150592634/eb6b029a-d41f-43e4-b362-f09cdebbec7c)



RAG, short for Retrieval-Augmented Generation, was first introduced by [Lewis et. al](http://arxiv.org/abs/2005.11401) in 2020. The authors introduced RAG as a novel approach to natural language processing (NLP) tasks that require access to external knowledge sources. RAG builds upon the advancements in large language models (LLMs) like GPT (Generative Pre-trained Transformer) models, integrating retrieval-based methods to enhance the generation process.

For beginners, understanding RAG starts with grasping the core concepts of retrieval-based and generation-based approaches in NLP. Retrieval-based methods involve retrieving relevant information from external knowledge sources, such as databases or pre-existing texts, to inform the generation process. On the other hand, generation-based methods focus on generating text based solely on the input prompt without explicit access to external knowledge.

RAG combines these two approaches by incorporating a retriever component that retrieves relevant context from external knowledge sources and a generator component that generates text conditioned on both the input prompt and the retrieved context. This fusion enables RAG to produce more informative and coherent responses compared to traditional generation models.

# Applications of RAG:

# Frameworks
[LangChain](https://www.langchain.com/)

[LlamaIndex](https://www.llamaindex.ai/)

[Haystack](https://haystack.deepset.ai/)

# Resources and Further Reading:

## Transformer:
[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[Visual introduction to Transformers](https://www.youtube.com/watch?v=ISPId9Lhc1g)

## RAG:

[Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](http://arxiv.org/abs/2005.11401)

[A Survey on Retrieval-Augmented Text Generation](https://arxiv.org/abs/2202.01110)

[Retrieval-Augmented Generation for Large Language Models: A Survey](http://arxiv.org/abs/2312.10997)

[Retrieval-Augmented Generation for AI-Generated Content: A Survey](https://arxiv.org/abs/2402.19473)


## LLM Stack

[LetsBuildAI](https://letsbuild.ai/)
